{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d63b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebbf111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>product_category</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>review_score</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>product_code</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>housewares</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4</td>\n",
       "      <td>1.506942e+09</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>housewares SKU 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>barreiras</td>\n",
       "      <td>BA</td>\n",
       "      <td>perfumery</td>\n",
       "      <td>1.0</td>\n",
       "      <td>118.70</td>\n",
       "      <td>4</td>\n",
       "      <td>1.532465e+09</td>\n",
       "      <td>595fac2a385ac33a80bd5114aec74eb8</td>\n",
       "      <td>perfumery SKU 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>vianopolis</td>\n",
       "      <td>GO</td>\n",
       "      <td>auto</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.90</td>\n",
       "      <td>5</td>\n",
       "      <td>1.533718e+09</td>\n",
       "      <td>aa4383b373c6aca5d8797843e5594415</td>\n",
       "      <td>auto SKU 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          order_id order_purchase_timestamp  \\\n",
       "0           0  e481f51cbdc54678b7cc49136f2d6af7      2017-10-02 10:56:33   \n",
       "1           1  53cdb2fc8bc7dce0b6741e2150273451      2018-07-24 20:41:37   \n",
       "2           2  47770eb9100c2d0c44946d9cf07ec65d      2018-08-08 08:38:49   \n",
       "\n",
       "                            user_id customer_city customer_state  \\\n",
       "0  9ef432eb6251297304e76186b10a928d     sao paulo             SP   \n",
       "1  b0830fb4747a6c6d20dea0b8c802d7ef     barreiras             BA   \n",
       "2  41ce2a54c0b03bf3443c3d931a367089    vianopolis             GO   \n",
       "\n",
       "  product_category  quantity   price  review_score     timestamp  \\\n",
       "0       housewares       1.0   29.99             4  1.506942e+09   \n",
       "1        perfumery       1.0  118.70             4  1.532465e+09   \n",
       "2             auto       1.0  159.90             5  1.533718e+09   \n",
       "\n",
       "                       product_code        product_id  \n",
       "0  87285b34884572647811a353c7ac498a  housewares SKU 0  \n",
       "1  595fac2a385ac33a80bd5114aec74eb8   perfumery SKU 0  \n",
       "2  aa4383b373c6aca5d8797843e5594415        auto SKU 0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterdf = pd.read_csv('../data_cleaned/brazildata_mod.csv')\n",
    "masterdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65aaaefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### standardize item data types, especially string, float, and integer\n",
    "\n",
    "masterdf[['user_id',      \n",
    "          'product_id',  \n",
    "         ]] = masterdf[['user_id','product_id']].astype(str)\n",
    "\n",
    "# we will play around with the data type of the quantity, \n",
    "# which you shall see later it affects the accuracy of the prediction.\n",
    "\n",
    "masterdf['quantity'] = masterdf['quantity'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8217c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define interactions data and user data\n",
    "\n",
    "### interactions \n",
    "### here we create a reference table of the user , item, and quantity purchased\n",
    "interactions_dict = masterdf.groupby(['user_id', 'product_id', 'timestamp'])[ 'quantity'].sum().reset_index()\n",
    "\n",
    "## we tansform the table inta a dictionary , which then we feed into tensor slices\n",
    "# this step is crucial as this will be the type of data fed into the embedding layers\n",
    "interactions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\n",
    "interactions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n",
    "\n",
    "## we do similar step for item, where this is the reference table for items to be recommended\n",
    "items_dict = masterdf[['product_id']].drop_duplicates()\n",
    "items_dict = {name: np.array(value) for name, value in items_dict.items()}\n",
    "items = tf.data.Dataset.from_tensor_slices(items_dict)\n",
    "\n",
    "## map the features in interactions and items to an identifier that we will use throught the embedding layers\n",
    "## do it for all the items in interaction and item table\n",
    "## you may often get itemtype error, so that is why here i am casting the quantity type as float to ensure consistency\n",
    "interactions = interactions.map(lambda x: {\n",
    "    'user_id' : x['user_id'], \n",
    "    'product_id' : x['product_id'], \n",
    "    'quantity' : float(x['quantity']),\n",
    "        \"timestamp\": x[\"timestamp\"]\n",
    "})\n",
    "\n",
    "items = items.map(lambda x: x['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60802e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic housekeeping to prepare feature vocabularies\n",
    "\n",
    "## timestamp is an exmaple of continuous features, which needs to be rescaled, or otherwise it will be \n",
    "## too large for the model.\n",
    "## there are other methods to reduce the size of the timestamp, ,such as standardization and normalization\n",
    "## here we use discretization, which puts them into buckets of categorical features, \n",
    "\n",
    "timestamps = np.concatenate(list(interactions.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,)\n",
    "\n",
    "item_titles = interactions.batch(10_000).map(lambda x: x[\"product_id\"])\n",
    "user_ids = interactions.batch(10_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_item_titles = np.unique(np.concatenate(list(item_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe715d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(60_000)\n",
    "test = shuffled.skip(60_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17840a72",
   "metadata": {},
   "source": [
    "# Multi-Task Model with ReLU-based DNN\n",
    "\n",
    "We can put it all together in a model class.\n",
    "\n",
    "The new component here is that - since we have two tasks and two losses - we need to decide on how important each loss is. We can do this by giving each of the losses a weight, and treating these weights as hyperparameters. If we assign a large loss weight to the rating task, our model is going to focus on predicting ratings (but still use some information from the retrieval task); if we assign a large loss weight to the retrieval task, it will focus on retrieval instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68622dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 rating_weight: float, retrieval_weight: float) -> None:\n",
    "        # We take the loss weights in the constructor: this allows us to instantiate\n",
    "        # several model objects with different loss weights.\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # item models.\n",
    "        self.item_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            vocabulary=unique_item_titles, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_item_titles) + 1, embedding_dimension)\n",
    "        ])\n",
    "            \n",
    "        ## user model    \n",
    "        self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # A small model to take in user and item embeddings and predict ratings.\n",
    "        # We can make this as complicated as we want as long as we output a scalar\n",
    "        # as our prediction.\n",
    "        \n",
    "        ## this is Relu-Based DNN\n",
    "        self.rating_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "\n",
    "        # rating and retrieval task.\n",
    "        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "        )\n",
    "            \n",
    "        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=items.batch(128).map(self.item_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # The loss weights.\n",
    "        self.rating_weight = rating_weight\n",
    "        self.retrieval_weight = retrieval_weight\n",
    "\n",
    "    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        \n",
    "        # And pick out the item features and pass them into the item model.\n",
    "        item_embeddings = self.item_model(features[\"product_id\"])\n",
    "\n",
    "        return (\n",
    "            user_embeddings,\n",
    "            item_embeddings,\n",
    "            # We apply the multi-layered rating model to a concatentation of\n",
    "            # user and item embeddings.\n",
    "            self.rating_model(\n",
    "                tf.concat([user_embeddings, item_embeddings], axis=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "        ## ratings go here as a method to compute loss\n",
    "        ratings = features.pop(\"quantity\")\n",
    "\n",
    "        user_embeddings, item_embeddings, rating_predictions = self(features)\n",
    "\n",
    "        # We compute the loss for each task.\n",
    "        rating_loss = self.rating_task(\n",
    "            labels=ratings,\n",
    "            predictions=rating_predictions,\n",
    "        )\n",
    "        retrieval_loss = self.retrieval_task(user_embeddings, item_embeddings)\n",
    "\n",
    "        # And combine them using the loss weights.\n",
    "        return (self.rating_weight * rating_loss\n",
    "                + self.retrieval_weight * retrieval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc74dc0",
   "metadata": {},
   "source": [
    "#### Rating-specialized model\n",
    "\n",
    "Depending on the weights we assign, the model will encode a different balance of the tasks. Let's start with a model that only considers ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d69ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8/8 [==============================] - 16s 1s/step - root_mean_squared_error: 2.5213 - factorized_top_k/top_1_categorical_accuracy: 1.0000e-04 - factorized_top_k/top_5_categorical_accuracy: 6.8333e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0013 - factorized_top_k/top_50_categorical_accuracy: 0.0058 - factorized_top_k/top_100_categorical_accuracy: 0.0113 - loss: 5.6863 - regularization_loss: 0.0000e+00 - total_loss: 5.6863\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 12s 1s/step - root_mean_squared_error: 2.4175 - factorized_top_k/top_1_categorical_accuracy: 1.1667e-04 - factorized_top_k/top_5_categorical_accuracy: 7.0000e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0012 - factorized_top_k/top_50_categorical_accuracy: 0.0058 - factorized_top_k/top_100_categorical_accuracy: 0.0113 - loss: 5.2687 - regularization_loss: 0.0000e+00 - total_loss: 5.2687\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 12s 2s/step - root_mean_squared_error: 2.4172 - factorized_top_k/top_1_categorical_accuracy: 6.6667e-05 - factorized_top_k/top_5_categorical_accuracy: 7.0000e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0013 - factorized_top_k/top_50_categorical_accuracy: 0.0058 - factorized_top_k/top_100_categorical_accuracy: 0.0114 - loss: 5.2672 - regularization_loss: 0.0000e+00 - total_loss: 5.2672\n",
      "4/4 [==============================] - 4s 560ms/step - root_mean_squared_error: 2.0218 - factorized_top_k/top_1_categorical_accuracy: 7.7012e-05 - factorized_top_k/top_5_categorical_accuracy: 8.4713e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0016 - factorized_top_k/top_50_categorical_accuracy: 0.0062 - factorized_top_k/top_100_categorical_accuracy: 0.0116 - loss: 4.1622 - regularization_loss: 0.0000e+00 - total_loss: 4.1622\n",
      "Retrieval top-100 accuracy: 0.012.\n",
      "Ranking RMSE: 2.022.\n"
     ]
    }
   ],
   "source": [
    "model = Model(rating_weight=1.0, retrieval_weight=0.0)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=3)\n",
    "metrics = model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
    "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe6f40",
   "metadata": {},
   "source": [
    "#### Retrieval-specialized model\n",
    "Let's now try a model that focuses on retrieval only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d34ac89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8/8 [==============================] - 14s 1s/step - root_mean_squared_error: 2.7940 - factorized_top_k/top_1_categorical_accuracy: 0.0013 - factorized_top_k/top_5_categorical_accuracy: 0.0058 - factorized_top_k/top_10_categorical_accuracy: 0.0089 - factorized_top_k/top_50_categorical_accuracy: 0.0201 - factorized_top_k/top_100_categorical_accuracy: 0.0300 - loss: 62066.9575 - regularization_loss: 0.0000e+00 - total_loss: 62066.9575\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 13s 2s/step - root_mean_squared_error: 2.7970 - factorized_top_k/top_1_categorical_accuracy: 0.0794 - factorized_top_k/top_5_categorical_accuracy: 0.1966 - factorized_top_k/top_10_categorical_accuracy: 0.2571 - factorized_top_k/top_50_categorical_accuracy: 0.4738 - factorized_top_k/top_100_categorical_accuracy: 0.6041 - loss: 61756.0530 - regularization_loss: 0.0000e+00 - total_loss: 61756.0530\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 16s 2s/step - root_mean_squared_error: 2.8055 - factorized_top_k/top_1_categorical_accuracy: 0.2115 - factorized_top_k/top_5_categorical_accuracy: 0.3760 - factorized_top_k/top_10_categorical_accuracy: 0.4408 - factorized_top_k/top_50_categorical_accuracy: 0.6635 - factorized_top_k/top_100_categorical_accuracy: 0.8037 - loss: 60031.4844 - regularization_loss: 0.0000e+00 - total_loss: 60031.4844\n",
      "4/4 [==============================] - 4s 905ms/step - root_mean_squared_error: 2.4839 - factorized_top_k/top_1_categorical_accuracy: 0.0022 - factorized_top_k/top_5_categorical_accuracy: 0.0115 - factorized_top_k/top_10_categorical_accuracy: 0.0205 - factorized_top_k/top_50_categorical_accuracy: 0.0558 - factorized_top_k/top_100_categorical_accuracy: 0.0809 - loss: 22256.6852 - regularization_loss: 0.0000e+00 - total_loss: 22256.6852\n",
      "Retrieval top-100 accuracy: 0.081.\n",
      "Ranking RMSE: 2.484.\n"
     ]
    }
   ],
   "source": [
    "model = Model(rating_weight=0.0, retrieval_weight=1.0)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "model.fit(cached_train, epochs=3)\n",
    "metrics = model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
    "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eee01b",
   "metadata": {},
   "source": [
    "#### Joint model\n",
    "\n",
    "Let's now train a model that assigns positive weights to both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8063a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8/8 [==============================] - 21s 2s/step - root_mean_squared_error: 2.4968 - factorized_top_k/top_1_categorical_accuracy: 6.1667e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0027 - factorized_top_k/top_10_categorical_accuracy: 0.0043 - factorized_top_k/top_50_categorical_accuracy: 0.0116 - factorized_top_k/top_100_categorical_accuracy: 0.0192 - loss: 31036.3785 - regularization_loss: 0.0000e+00 - total_loss: 31036.3785\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 20s 2s/step - root_mean_squared_error: 2.4170 - factorized_top_k/top_1_categorical_accuracy: 0.0237 - factorized_top_k/top_5_categorical_accuracy: 0.0753 - factorized_top_k/top_10_categorical_accuracy: 0.1075 - factorized_top_k/top_50_categorical_accuracy: 0.2323 - factorized_top_k/top_100_categorical_accuracy: 0.3102 - loss: 30991.6584 - regularization_loss: 0.0000e+00 - total_loss: 30991.6584\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 20s 2s/step - root_mean_squared_error: 2.4162 - factorized_top_k/top_1_categorical_accuracy: 0.0896 - factorized_top_k/top_5_categorical_accuracy: 0.1976 - factorized_top_k/top_10_categorical_accuracy: 0.2564 - factorized_top_k/top_50_categorical_accuracy: 0.5053 - factorized_top_k/top_100_categorical_accuracy: 0.6648 - loss: 30816.6330 - regularization_loss: 0.0000e+00 - total_loss: 30816.6330\n",
      "4/4 [==============================] - 5s 920ms/step - root_mean_squared_error: 2.0204 - factorized_top_k/top_1_categorical_accuracy: 0.0027 - factorized_top_k/top_5_categorical_accuracy: 0.0133 - factorized_top_k/top_10_categorical_accuracy: 0.0220 - factorized_top_k/top_50_categorical_accuracy: 0.0591 - factorized_top_k/top_100_categorical_accuracy: 0.0826 - loss: 11135.5904 - regularization_loss: 0.0000e+00 - total_loss: 11135.5904\n",
      "Retrieval top-100 accuracy: 0.083.\n",
      "Ranking RMSE: 2.020.\n"
     ]
    }
   ],
   "source": [
    "model = Model(rating_weight=0.5, retrieval_weight=0.5)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "model.fit(cached_train, epochs=3)\n",
    "metrics = model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
    "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f45af",
   "metadata": {},
   "source": [
    "We can see that accuracy is highest and RMSE is lowest when we combine both ranking and retrieval together.\n",
    "\n",
    "But so far we only have timestamp as additional features. How can we incorporate all of them into one single model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b5248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow 2.0)",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
