{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999437b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876f507c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>product_category</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>review_score</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>product_code</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>housewares</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4</td>\n",
       "      <td>1.506942e+09</td>\n",
       "      <td>87285b34884572647811a353c7ac498a</td>\n",
       "      <td>housewares SKU 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>barreiras</td>\n",
       "      <td>BA</td>\n",
       "      <td>perfumery</td>\n",
       "      <td>1.0</td>\n",
       "      <td>118.70</td>\n",
       "      <td>4</td>\n",
       "      <td>1.532465e+09</td>\n",
       "      <td>595fac2a385ac33a80bd5114aec74eb8</td>\n",
       "      <td>perfumery SKU 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>vianopolis</td>\n",
       "      <td>GO</td>\n",
       "      <td>auto</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.90</td>\n",
       "      <td>5</td>\n",
       "      <td>1.533718e+09</td>\n",
       "      <td>aa4383b373c6aca5d8797843e5594415</td>\n",
       "      <td>auto SKU 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          order_id order_purchase_timestamp  \\\n",
       "0           0  e481f51cbdc54678b7cc49136f2d6af7      2017-10-02 10:56:33   \n",
       "1           1  53cdb2fc8bc7dce0b6741e2150273451      2018-07-24 20:41:37   \n",
       "2           2  47770eb9100c2d0c44946d9cf07ec65d      2018-08-08 08:38:49   \n",
       "\n",
       "                            user_id customer_city customer_state  \\\n",
       "0  9ef432eb6251297304e76186b10a928d     sao paulo             SP   \n",
       "1  b0830fb4747a6c6d20dea0b8c802d7ef     barreiras             BA   \n",
       "2  41ce2a54c0b03bf3443c3d931a367089    vianopolis             GO   \n",
       "\n",
       "  product_category  quantity   price  review_score     timestamp  \\\n",
       "0       housewares       1.0   29.99             4  1.506942e+09   \n",
       "1        perfumery       1.0  118.70             4  1.532465e+09   \n",
       "2             auto       1.0  159.90             5  1.533718e+09   \n",
       "\n",
       "                       product_code        product_id  \n",
       "0  87285b34884572647811a353c7ac498a  housewares SKU 0  \n",
       "1  595fac2a385ac33a80bd5114aec74eb8   perfumery SKU 0  \n",
       "2  aa4383b373c6aca5d8797843e5594415        auto SKU 0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterdf = pd.read_csv('../data_cleaned/brazildata_mod.csv')\n",
    "masterdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55543fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### standardize item data types, especially string, float, and integer\n",
    "\n",
    "masterdf[['user_id',      \n",
    "          'product_id',  \n",
    "         ]] = masterdf[['user_id','product_id']].astype(str)\n",
    "\n",
    "# we will play around with the data type of the quantity, \n",
    "# which you shall see later it affects the accuracy of the prediction.\n",
    "\n",
    "masterdf['quantity'] = masterdf['quantity'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce7cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_dict = masterdf.groupby(['user_id', 'product_id', 'timestamp'])[ 'quantity'].sum().reset_index()\n",
    "\n",
    "interactions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\n",
    "interactions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n",
    "\n",
    "items_dict = masterdf[['product_id']].drop_duplicates()\n",
    "items_dict = {name: np.array(value) for name, value in items_dict.items()}\n",
    "items = tf.data.Dataset.from_tensor_slices(items_dict)\n",
    "\n",
    "interactions = interactions.map(lambda x: {\n",
    "                                            'user_id' : x['user_id'], \n",
    "                                            'product_id' : x['product_id'], \n",
    "                                            'quantity' : float(x['quantity']),\n",
    "                                            \"timestamp\": x[\"timestamp\"] })\n",
    "\n",
    "items = items.map(lambda x: x['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc0a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic housekeeping to prepare feature vocabularies\n",
    "\n",
    "## timestamp is an exmaple of continuous features, which needs to be rescaled, or otherwise it will be \n",
    "## too large for the model.\n",
    "## there are other methods to reduce the size of the timestamp, ,such as standardization and normalization\n",
    "## here we use discretization, which puts them into buckets of categorical features, \n",
    "\n",
    "timestamps = np.concatenate(list(interactions.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,)\n",
    "\n",
    "item_titles = interactions.batch(10_000).map(lambda x: x[\"product_id\"])\n",
    "user_ids = interactions.batch(10_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_item_titles = np.unique(np.concatenate(list(item_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd13fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(60_000)\n",
    "test = shuffled.skip(60_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f37ac",
   "metadata": {},
   "source": [
    "# Model implementation\n",
    "\n",
    "We split the query and candidate model separately to allow more stacked embedding layers before we pass it into the model.\n",
    "\n",
    "In the user model (query model), in addition to user embedding, we also add timestamp embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89eb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "### user model\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "\n",
    "        self._use_timestamps = use_timestamps\n",
    "\n",
    "        ## embed user id from unique_user_ids\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "        ])\n",
    "\n",
    "        ## embed timestamp\n",
    "        if use_timestamps:\n",
    "            self.timestamp_embedding = tf.keras.Sequential([\n",
    "              tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "              tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "            ])\n",
    "            self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "            self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not self._use_timestamps:\n",
    "              return self.user_embedding(inputs[\"user_id\"])\n",
    "\n",
    "        ## all features here\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "            self.normalized_timestamp(inputs[\"timestamp\"]),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d26b1",
   "metadata": {},
   "source": [
    "For the candidate model, we want the model to learn from the text features too by processing the text features that is able to learn words that are similar to each other. It can also identify OOV (out of Vocabulary) word, so if we are predicing a new item, the model can calculate them appropriately.\n",
    "\n",
    "Below, the item name will be transformated by tokenization (splitting into constituent words or word-pieces), followed by vocabulary learning, then followed by an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35523fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### candidate model\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000\n",
    "\n",
    "        ## embed title from unique_item_titles\n",
    "        self.title_embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "              vocabulary=unique_item_titles, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_item_titles) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        ## processing text features: item title vectorizer (see self.title_vectorizer)\n",
    "        self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        ## we apply title vectorizer to items\n",
    "        self.title_text_embedding = tf.keras.Sequential([\n",
    "          self.title_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.title_vectorizer.adapt(items)\n",
    "\n",
    "    def call(self, titles):\n",
    "        return tf.concat([\n",
    "            self.title_embedding(titles),\n",
    "            self.title_text_embedding(titles),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e4f3f",
   "metadata": {},
   "source": [
    "With both UserModel and ItemModel defined, we can put together a combined model and implement our loss and metrics logic.\n",
    "\n",
    "Note that we also need to make sure that the query model and candidate model output embeddings of compatible size. Because we'll be varying their sizes by adding more features, the easiest way to accomplish this is to use a dense projection layer after each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5157f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetailModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## query model is user model\n",
    "        self.query_model = tf.keras.Sequential([\n",
    "          UserModel(use_timestamps),\n",
    "          tf.keras.layers.Dense(32)\n",
    "        ])\n",
    "        \n",
    "        ## candidate model is the item model\n",
    "        self.candidate_model = tf.keras.Sequential([\n",
    "          ItemModel(),\n",
    "          tf.keras.layers.Dense(32)\n",
    "        ])\n",
    "        \n",
    "        ## retrieval task, choose metrics\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=items.batch(128).map(self.candidate_model),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # We only pass the user id and timestamp features into the query model. This\n",
    "        # is to ensure that the training inputs would have the same keys as the\n",
    "        # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "        # error when loading the query model after saving it.\n",
    "        \n",
    "        query_embeddings = self.query_model({\n",
    "            \"user_id\": features[\"user_id\"],\n",
    "            \"timestamp\": features[\"timestamp\"],\n",
    "        })\n",
    "        \n",
    "        item_embeddings = self.candidate_model(features[\"product_id\"])\n",
    "\n",
    "        return self.task(query_embeddings, item_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f004d1",
   "metadata": {},
   "source": [
    "Baseline: no timestamp features\n",
    "\n",
    "We're ready to try out our first model: let's start with not using timestamp features to establish our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b00adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "30/30 [==============================] - 15s 401ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0032 - factorized_top_k/top_5_categorical_accuracy: 0.0100 - factorized_top_k/top_10_categorical_accuracy: 0.0139 - factorized_top_k/top_50_categorical_accuracy: 0.0284 - factorized_top_k/top_100_categorical_accuracy: 0.0378 - loss: 14858.1789 - regularization_loss: 0.0000e+00 - total_loss: 14858.1789\n",
      "Epoch 2/3\n",
      "30/30 [==============================] - 13s 407ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0217 - factorized_top_k/top_5_categorical_accuracy: 0.0758 - factorized_top_k/top_10_categorical_accuracy: 0.1067 - factorized_top_k/top_50_categorical_accuracy: 0.2140 - factorized_top_k/top_100_categorical_accuracy: 0.2801 - loss: 12659.2235 - regularization_loss: 0.0000e+00 - total_loss: 12659.2235\n",
      "Epoch 3/3\n",
      "30/30 [==============================] - 14s 421ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1001 - factorized_top_k/top_5_categorical_accuracy: 0.3038 - factorized_top_k/top_10_categorical_accuracy: 0.3794 - factorized_top_k/top_50_categorical_accuracy: 0.5654 - factorized_top_k/top_100_categorical_accuracy: 0.6522 - loss: 8366.2898 - regularization_loss: 0.0000e+00 - total_loss: 8366.2898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f147ef66a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RetailModel(use_timestamps=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model.fit(cached_train, epochs=3)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b973958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "4/4 [==============================] - 4s 615ms/step - factorized_top_k/top_1_categorical_accuracy: 2.3104e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0023 - factorized_top_k/top_10_categorical_accuracy: 0.0035 - factorized_top_k/top_50_categorical_accuracy: 0.0097 - factorized_top_k/top_100_categorical_accuracy: 0.0166 - loss: 24718.6561 - regularization_loss: 0.0000e+00 - total_loss: 24718.6561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.00023103581042960286,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0023103582207113504,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.003465537214651704,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.009703503921627998,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.016557566821575165,\n",
       " 'loss': 5160.34521484375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 5160.34521484375}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a4f7a26",
   "metadata": {},
   "source": [
    "Including time into the model:\n",
    "\n",
    "Do the result change if we add time features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f4dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
      "30/30 [==============================] - 17s 471ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0083 - factorized_top_k/top_5_categorical_accuracy: 0.0133 - factorized_top_k/top_10_categorical_accuracy: 0.0174 - factorized_top_k/top_50_categorical_accuracy: 0.0361 - factorized_top_k/top_100_categorical_accuracy: 0.0544 - loss: 14498.4631 - regularization_loss: 0.0000e+00 - total_loss: 14498.4631\n",
      "Epoch 2/3\n",
      "30/30 [==============================] - 15s 476ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0158 - factorized_top_k/top_5_categorical_accuracy: 0.0511 - factorized_top_k/top_10_categorical_accuracy: 0.0774 - factorized_top_k/top_50_categorical_accuracy: 0.1855 - factorized_top_k/top_100_categorical_accuracy: 0.2624 - loss: 12165.9211 - regularization_loss: 0.0000e+00 - total_loss: 12165.9211\n",
      "Epoch 3/3\n",
      "30/30 [==============================] - 15s 482ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0954 - factorized_top_k/top_5_categorical_accuracy: 0.2935 - factorized_top_k/top_10_categorical_accuracy: 0.3930 - factorized_top_k/top_50_categorical_accuracy: 0.6407 - factorized_top_k/top_100_categorical_accuracy: 0.7464 - loss: 7247.7276 - regularization_loss: 0.0000e+00 - total_loss: 7247.7276\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
      "4/4 [==============================] - 3s 660ms/step - factorized_top_k/top_1_categorical_accuracy: 6.1610e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0039 - factorized_top_k/top_10_categorical_accuracy: 0.0065 - factorized_top_k/top_50_categorical_accuracy: 0.0214 - factorized_top_k/top_100_categorical_accuracy: 0.0360 - loss: 25326.7271 - regularization_loss: 0.0000e+00 - total_loss: 25326.7271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.000616095494478941,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.003850596956908703,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.006546014454215765,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.02140931785106659,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.03596457466483116,\n",
       " 'loss': 5379.62255859375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 5379.62255859375}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  RetailModel(use_timestamps=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "model.fit(cached_train, epochs=3)\n",
    "\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ae0ce",
   "metadata": {},
   "source": [
    "Eventhough we only run it at three epochs, we can see accuracy increase as we add time into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90312bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow 2.0)",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
